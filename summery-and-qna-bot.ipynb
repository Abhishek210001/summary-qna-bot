{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11378730,"sourceType":"datasetVersion","datasetId":7124281}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install pymupdf\n\nfrom transformers import pipeline\nimport fitz  # PyMuPDF\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-17T13:18:52.625750Z","iopub.execute_input":"2025-07-17T13:18:52.625952Z","iopub.status.idle":"2025-07-17T13:19:38.298167Z","shell.execute_reply.started":"2025-07-17T13:18:52.625935Z","shell.execute_reply":"2025-07-17T13:19:38.297571Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nCollecting pymupdf\n  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\nDownloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pymupdf\nSuccessfully installed pymupdf-1.26.3\n","output_type":"stream"},{"name":"stderr","text":"2025-07-17 13:19:16.423054: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752758356.787887      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752758356.891360      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Load summarization model\nsummarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n\n# Load question answering model\nqa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-large-squad2\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T13:19:38.298839Z","iopub.execute_input":"2025-07-17T13:19:38.299275Z","iopub.status.idle":"2025-07-17T13:20:03.381030Z","shell.execute_reply.started":"2025-07-17T13:19:38.299253Z","shell.execute_reply":"2025-07-17T13:20:03.380452Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fc13223de59449c883ede2612491ad5"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6cab7297e2d4368864fdbfa568abbee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"090aeeb2c5a24819a74538049ea13dff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b222d120a75410d9117f92cc5b295d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc3d803a4e7e44c0bb7fa99e8551d621"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af231c08252f4bfcbe3189d566b75c26"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/696 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc0c388433964f3da6c3594267948068"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52aa963117cd4db2b5bbd1e82719834e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99d87ff2027a4ded98f3fbbdc13fcf2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"044cf1631266405f8879596f7180f5e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08a6ee57be3c432e8a5f8ce45acaeaff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7952fe55575946a88d8d58491686327a"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def extract_text_from_pdf(pdf_path):\n    \"\"\"Extracts all text from PDF.\"\"\"\n    doc = fitz.open(pdf_path)\n    return \"\\n\".join(page.get_text(\"text\") for page in doc)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T13:20:03.382522Z","iopub.execute_input":"2025-07-17T13:20:03.382767Z","iopub.status.idle":"2025-07-17T13:20:03.386754Z","shell.execute_reply.started":"2025-07-17T13:20:03.382749Z","shell.execute_reply":"2025-07-17T13:20:03.386056Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def calculate_similarity(question, paragraph):\n    question_words = set(question.lower().split())\n    para_words = set(paragraph.lower().split())\n    common_words = question_words.intersection(para_words)\n    \n    tech_term_weight = sum(3 for word in common_words if word in [\n        \"dataset\", \"preprocessing\", \"model\", \"normalize\", \"resolution\", \n        \"segmentation\", \"training\"\n    ])\n    \n    return len(common_words) / (len(question_words) + len(para_words) - len(common_words)) + tech_term_weight\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T13:20:03.390368Z","iopub.execute_input":"2025-07-17T13:20:03.390854Z","iopub.status.idle":"2025-07-17T13:20:03.405350Z","shell.execute_reply.started":"2025-07-17T13:20:03.390830Z","shell.execute_reply":"2025-07-17T13:20:03.404820Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def find_relevant_paragraphs(text, question, num_paragraphs=2):\n    paragraphs = text.split('\\n\\n')\n    scored = []\n    \n    for para in paragraphs:\n        if len(para.strip()) < 10:\n            continue\n        score = calculate_similarity(question, para)\n        scored.append((para, score))\n    \n    scored.sort(key=lambda x: x[1], reverse=True)\n    return [p[0] for p in scored[:num_paragraphs]]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T13:20:03.405977Z","iopub.execute_input":"2025-07-17T13:20:03.406181Z","iopub.status.idle":"2025-07-17T13:20:03.424149Z","shell.execute_reply.started":"2025-07-17T13:20:03.406166Z","shell.execute_reply":"2025-07-17T13:20:03.423639Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def extract_comprehensive_steps(context, initial_answer):\n    indicators = [\"preprocessing\", \"normalization\", \"resiz\", \"organiz\", \"segment\", \n                  \"convert\", \"structur\", \"divid\", \"standard\"]\n    steps = []\n    \n    for para in context.split('\\n'):\n        if any(ind in para.lower() for ind in indicators):\n            for sentence in para.split('.'):\n                if any(ind in sentence.lower() for ind in indicators):\n                    clean = sentence.strip()\n                    if clean and len(clean) > 10:\n                        steps.append(clean)\n                        \n    return \"The preprocessing steps included: \" + \"; \".join(steps) if steps else initial_answer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T13:20:03.424731Z","iopub.execute_input":"2025-07-17T13:20:03.424903Z","iopub.status.idle":"2025-07-17T13:20:03.442458Z","shell.execute_reply.started":"2025-07-17T13:20:03.424891Z","shell.execute_reply":"2025-07-17T13:20:03.441742Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def expand_what_answer(answer, context, question):\n    \"\"\"\n    Expands short 'what' answers by pulling more descriptive sentences from the context.\n    \"\"\"\n    # Split context into sentences\n    sentences = context.split(\".\")\n    \n    # Try to find sentences with key words from the question\n    question_keywords = question.lower().split()\n    best_sentences = []\n    \n    for sentence in sentences:\n        score = sum(1 for word in question_keywords if word in sentence.lower())\n        if score >= 2:  # Sentence must match at least 2 question words\n            best_sentences.append(sentence.strip())\n    \n    # Return the best match with original answer as fallback\n    return \". \".join(best_sentences[:2]) if best_sentences else answer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T13:20:03.443136Z","iopub.execute_input":"2025-07-17T13:20:03.443379Z","iopub.status.idle":"2025-07-17T13:20:03.463693Z","shell.execute_reply.started":"2025-07-17T13:20:03.443318Z","shell.execute_reply":"2025-07-17T13:20:03.463178Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def enhance_answer(answer, question, context):\n    if len(answer.split()) < 5:\n        if \"and\" in question or \",\" in question:\n            if \"what\" in question.lower():\n                return expand_what_answer(answer, context, question)\n            \n            if \"which\" in question.lower() and \"format\" in question.lower():\n                if \"dataset\" in answer.lower() and \"format\" not in answer.lower():\n                    return f\"{answer} stored in .nii.gz format\"\n                if \"format\" in answer.lower() and \"dataset\" not in answer.lower():\n                    return f\"MMWHS dataset with {answer}\"\n    \n    if \"preprocessing\" in question.lower() or \"steps\" in question.lower():\n        return extract_comprehensive_steps(context, answer)\n    \n    return answer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T13:20:03.465365Z","iopub.execute_input":"2025-07-17T13:20:03.465598Z","iopub.status.idle":"2025-07-17T13:20:03.482137Z","shell.execute_reply.started":"2025-07-17T13:20:03.465576Z","shell.execute_reply":"2025-07-17T13:20:03.481623Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def process_input(input_data, is_pdf=False, question=None):\n    if is_pdf:\n        text = extract_text_from_pdf(input_data)\n    else:\n        text = input_data\n\n    text_for_summary = text[:2048] if len(text) > 2048 else text\n    summary = summarizer(text_for_summary, max_length=200, min_length=100, do_sample=False)[0]['summary_text']\n\n    if question:\n        relevant_paras = find_relevant_paragraphs(text, question, num_paragraphs=2)\n        extended_context = summary + \"\\n\\n\" + \"\\n\".join(relevant_paras)\n\n        answer_raw = qa_pipeline(\n            question=question, \n            context=extended_context\n        )['answer']\n\n        answer_raw = enhance_answer(answer_raw, question, extended_context)\n        full_answer = f\"The answer to the question '{question}' is: {answer_raw}.\"\n        \n        eval_info = {\n            \"Answer Length\": len(answer_raw),\n            \"Is Short Answer\": len(answer_raw.split()) < 10,\n            \"Contains Key Phrase\": any(k in answer_raw.lower() for k in [\n                \"dataset\", \"format\", \"tool\", \"model\", \"architecture\", \n                \"preprocessing\", \"training\", \"resolution\", \"normalization\"\n            ])\n        }\n\n        return {\n            \"Summary\": summary,\n            \"QnA Answer\": full_answer,\n            \"Evaluation\": eval_info\n        }\n\n    return {\"Summary\": summary}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T13:20:03.482724Z","iopub.execute_input":"2025-07-17T13:20:03.482963Z","iopub.status.idle":"2025-07-17T13:20:03.505195Z","shell.execute_reply.started":"2025-07-17T13:20:03.482943Z","shell.execute_reply":"2025-07-17T13:20:03.504711Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"pdf_path = \"/kaggle/input/contribution/Abhishek_Contribution_Extended.pdf\"\n\n# Example test without question\nprint(process_input(pdf_path, is_pdf=True))\n\n# Example test with question\nquestion = \"Which deep learning model was used in the project and why?\"\nprint(process_input(pdf_path, is_pdf=True, question=question))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T13:20:03.505854Z","iopub.execute_input":"2025-07-17T13:20:03.506054Z","iopub.status.idle":"2025-07-17T13:20:08.686115Z","shell.execute_reply.started":"2025-07-17T13:20:03.506039Z","shell.execute_reply":"2025-07-17T13:20:08.685283Z"}},"outputs":[{"name":"stdout","text":"{'Summary': 'The project focused on detecting Coronary Artery Calcification (CAC) using deep learning methods. For this purpose, we used CT scan images from the MMWHS dataset. Abhishek Kumar was involved in collecting, organizing, and preprocessing the images. The dataset was divided into two parts: a Training Set and a Testing Set. The Training Set is used to teach the model and the Testing Set to evaluate how well the model performs on unseen data. The Pre-processed Dataset was used for training the model.'}\n{'Summary': 'The project focused on detecting Coronary Artery Calcification (CAC) using deep learning methods. For this purpose, we used CT scan images from the MMWHS dataset. Abhishek Kumar was involved in collecting, organizing, and preprocessing the images. The dataset was divided into two parts: a Training Set and a Testing Set. The Training Set is used to teach the model and the Testing Set to evaluate how well the model performs on unseen data. The Pre-processed Dataset was used for training the model.', 'QnA Answer': \"The answer to the question 'Which deep learning model was used in the project and why?' is: advanced deep learning models.\", 'Evaluation': {'Answer Length': 29, 'Is Short Answer': True, 'Contains Key Phrase': True}}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"","metadata":{}}]}